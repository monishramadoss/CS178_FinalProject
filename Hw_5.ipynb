{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent import futures\n",
    "import nltk\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from scipy import sparse\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "NUM_THREADS = 24\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "reuters_freqThreshold = 1\n",
    "shakespeare_freqThreshold = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuters Data Scrubbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(reuters.fileids()))\n",
    "punctuation = string.punctuation.replace(\"'\", \"\")\n",
    "stopset = set(stopwords.words(\"english\"))\n",
    "\n",
    "def thread_task(rawWords):\n",
    "    freqMap = defaultdict(int)\n",
    "    #convert to lowercase\n",
    "    lower = [word.lower() for word in rawWords]\n",
    "    \n",
    "    #remove punctuation from tokens\n",
    "    punc_filter = str.maketrans('', '', punctuation)\n",
    "    stripped = [word.translate(punc_filter) for word in lower]\n",
    "    \n",
    "    #remove remaining alphanumerics\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    #filter out basic stopwords\n",
    "    cleaned_word_bank = [word for word in words if word not in stopset]\n",
    "    \n",
    "    for word in cleaned_word_bank:\n",
    "        freqMap[word] += 1\n",
    "    return freqMap\n",
    "\n",
    "def thread_exec(WORDS):\n",
    "    with futures.ThreadPoolExecutor(max_workers=NUM_THREADS) as ex:\n",
    "        results = list(ex.map(thread_task, WORDS))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqMap = defaultdict(int)\n",
    "REUTERS_WORDS = [[str(word) for word in reuters.words(file)] for file in reuters.fileids()][0:10000]\n",
    "t = thread_exec(REUTERS_WORDS)\n",
    "for ifreqMap in t:\n",
    "    for k in ifreqMap.keys():    \n",
    "        freqMap[k] += ifreqMap[k]    \n",
    "        \n",
    "freqTuples = list(freqMap.items())\n",
    "sorted_freq = freqTuples.sort(key= lambda x: x[1], reverse=True)\n",
    "topTuples_r = freqTuples[:reuters_freqThreshold]\n",
    "filtered_vocab = [tup[0] for tup in topTuples_r]\n",
    "reuters_vocab_np = np.asarray(filtered_vocab, dtype='str')\n",
    "print(reuters_vocab_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shakespeare Data Scrubbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './Shake_works'\n",
    "sub = \"[^a-zA-Z' ]+\"\n",
    "all_files = [file for t,y, file in os.walk(folder)][0]\n",
    "stopset = set(stopwords.words(\"english\"))\n",
    "\n",
    "def thread_task(rawWords):\n",
    "    freqMap = defaultdict(int)\n",
    "    words = [re.sub(sub, '', word) for word in rawWords]\n",
    "    lower = [word.lower() for word in words]\n",
    "    words = [word for word in lower if word.isalpha()]\n",
    "    cleaned_word_bank = [word for word in words if word not in stopset]    \n",
    "    \n",
    "    for word in cleaned_word_bank:\n",
    "        freqMap[word] += 1\n",
    "    return freqMap\n",
    "\n",
    "def thread_exec(WORDS):\n",
    "    with futures.ThreadPoolExecutor(max_workers=NUM_THREADS) as ex:\n",
    "        results = list(ex.map(thread_task, WORDS))\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqMap = defaultdict(int)\n",
    "SHAKESPEARE_WORDS = [[str(word) for word in open(folder+'/'+file).read().split()] for file in all_files]\n",
    "t = thread_exec(SHAKESPEARE_WORDS)\n",
    "for ifreqMap in t:\n",
    "    for k in ifreqMap.keys():    \n",
    "        freqMap[k] += ifreqMap[k]   \n",
    "        \n",
    "freqTuples = list(freqMap.items())\n",
    "sorted_freq = freqTuples.sort(key= lambda x: x[1], reverse=True)\n",
    "topTuples_s = freqTuples[:shakespeare_freqThreshold]\n",
    "filtered_vocab = [tup[0] for tup in topTuples_s]\n",
    "shakespeare_vocab_np = np.asarray(filtered_vocab, dtype='str')\n",
    "print(shakespeare_vocab_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputting top words/frequencies to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_freq_file = open(\"reuters_frequencies.txt\", \"w+\")\n",
    "shakes_freq_file = open(\"shakes_frequencies.txt\", \"w+\")\n",
    "\n",
    "for tup in topTuples_r:\n",
    "    reuters_freq_file.write(f\"{tup[0]}, {tup[1]}\")\n",
    "for tup in topTuples_s:\n",
    "    shakes_freq_file.write(f\"{tup[0]}, {tup[1]}\")\n",
    "    \n",
    "reuters_freq_file.close()\n",
    "shakes_freq_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress, HTML, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "vocab = reuters_vocab_np\n",
    "tmp = list()\n",
    "for word in shakespeare_vocab_np:\n",
    "    if(word not in vocab):\n",
    "        tmp.append(word)\n",
    "vocab = np.append(vocab, tmp)\n",
    "print(vocab.shape)\n",
    "\n",
    "\n",
    "shakespeare_data = []\n",
    "i=0\n",
    "punc_filter = str.maketrans('', '', punctuation)\n",
    "progress = IntProgress(min=0, max=len(SHAKESPEARE_WORDS))\n",
    "label = HTML()\n",
    "box = VBox(children=[label, progress])\n",
    "display(box)\n",
    "for doc in SHAKESPEARE_WORDS:\n",
    "    shakespeare_data.append(np.zeros(vocab.shape))\n",
    "    #print(doc)\n",
    "    for word in doc:\n",
    "        word = re.sub(sub,'',word).lower()\n",
    "        shakespeare_data[i][vocab==word] = 1.0\n",
    "    i+=1\n",
    "    progress.value += 1\n",
    "    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=\"Docs\",\n",
    "                        index=i,\n",
    "                        size=len(SHAKESPEARE_WORDS)\n",
    "                    )\n",
    "shakespeare_data = np.array(shakespeare_data)   \n",
    "\n",
    "\n",
    "reuters_data = []\n",
    "i=0\n",
    "punc_filter = str.maketrans('', '', punctuation)\n",
    "progress = IntProgress(min=0, max=len(REUTERS_WORDS))\n",
    "label = HTML()\n",
    "box = VBox(children=[label, progress])\n",
    "display(box)\n",
    "for doc in REUTERS_WORDS:\n",
    "    reuters_data.append(np.zeros(vocab.shape))\n",
    "    for word in doc:\n",
    "        word = word.lower()\n",
    "        word = word.translate(punc_filter)\n",
    "        reuters_data[i][vocab==word] = 1.0\n",
    "    i+=1   \n",
    "    progress.value += 1\n",
    "    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=\"Docs\",\n",
    "                        index=i,\n",
    "                        size=len(REUTERS_WORDS)\n",
    "                    )\n",
    "reuters_data = np.array(reuters_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = shakespeare_data\n",
    "y = np.zeros(X.shape[0])\n",
    "\n",
    "X = np.concatenate((X, reuters_data), axis=0)\n",
    "y = np.concatenate((y, np.ones(reuters_data.shape[0])), axis=0)\n",
    "\n",
    "vocab = np.array(vocab, dtype=np.str)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(vocab.shape)\n",
    "\n",
    "X.dump('X')\n",
    "y.dump('y')\n",
    "vocab.dump('vocab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('X')\n",
    "y = np.load('y')\n",
    "vocab = np.load('vocab')\n",
    "models = [(BernoulliNB(), \"Bernoulli Naive Bayes\"), \n",
    "          (GaussianNB(), \"Gaussian Naive Bayes\"),\n",
    "          (LogisticRegression(), \"Logistic Regression\"),\n",
    "          (KNeighborsClassifier(n_neighbors=5), \"K Nearest Neighbours\"),\n",
    "          (svm.SVC(), \"Support Vector Machine\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(l1,l2):\n",
    "    precisionError = len([b for a,b in zip(l1,l2) if b != a and b == 0 and a == 1])\n",
    "    recallError = len([b for a,b in zip(l1,l2) if b != a and b == 1 and a == 0])\n",
    "    print(\"\\tPrecision Error is \" + str(precisionError))\n",
    "    print(\"\\tRecall Error is \" + str(recallError))\n",
    "\n",
    "    return (precisionError) + (recallError)\n",
    "\n",
    "def runModels(clf, Xtr, Ytr):\n",
    "    kf = KFold(n_splits=5)    \n",
    "    differencesB = []\n",
    "    differencesG = []\n",
    "\n",
    "    for train_index, test_index in kf.split(Xtr):\n",
    "        x_train, x_test = [Xtr[i] for i in train_index], [Xtr[i] for i in test_index]\n",
    "        y_train, y_test = [Ytr[i] for i in train_index], [Ytr[i] for i in test_index]    \n",
    "        clf.fit(x_train,y_train)\n",
    "        yhat = clf.predict(x_test)\n",
    "        mse = ((y_test-yhat)**2).mean(axis = 0)\n",
    "        print(\"\\tMSE: \", mse)\n",
    "        differencesB.append(difference(clf.predict(x_test), y_test) / len(y_test))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf, name in models:\n",
    "    print(name)\n",
    "    try:\n",
    "        runModels(clf, X, y)\n",
    "    except:\n",
    "        print(\"\\t error training model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_error = []\n",
    "print(\"seperating start\")\n",
    "X_s = X[y==0]\n",
    "X_r = X[y==1]\n",
    "\n",
    "Y_s = np.zeros(X_s.shape[0])\n",
    "Y_r = np.ones(X_r.shape[0])\n",
    "\n",
    "print(\"seperated docs\")\n",
    "\n",
    "## 5 -> 8 shakespere docs\n",
    "## 5 -> ~2k reuters docs\n",
    "folds = KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "splits = []\n",
    "training_sx = []\n",
    "training_sy = []\n",
    "validation_sx = []\n",
    "validation_sy = []\n",
    "\n",
    "training_rx = []\n",
    "training_ry = []\n",
    "validation_rx = []\n",
    "validation_ry = []\n",
    "\n",
    "differencesB = []\n",
    "\n",
    "print(\"splitting shakespere\")\n",
    "for train_index, test_index in folds.split(X_s, Y_s):\n",
    "    x_tr, x_validation = X_s[train_index], X_s[test_index]\n",
    "    y_tr, y_validation = Y_s[train_index], Y_s[test_index]\n",
    "    training_sx.append(x_tr)\n",
    "    training_sy.append(y_tr)\n",
    "    validation_sx.append(x_validation) \n",
    "    validation_sy.append(y_validation)\n",
    "print(\"split shakespeare done\")\n",
    "    \n",
    "for train_index, test_index in folds.split(X_r, Y_r):\n",
    "    x_tr, x_validation = X_r[train_index], X_r[test_index]\n",
    "    y_tr, y_validation = Y_r[train_index], Y_r[test_index]\n",
    "    training_rx.append(x_tr)\n",
    "    training_ry.append(y_tr)\n",
    "    validation_rx.append(x_validation) \n",
    "    validation_ry.append(y_validation)\n",
    "print(\"split shakespere done\")\n",
    "\n",
    "print(\"training\")\n",
    "\n",
    "print(training_rx[0])\n",
    "def validate(clf, name):\n",
    "    for i in range(len(training_sx)):\n",
    "        training_global_x = (np.concatenate((training_sx[i], training_rx[i]), axis=0))\n",
    "        training_global_y = (np.concatenate((training_sy[i], training_ry[i]), axis=0))\n",
    "        validation_global_x = (np.concatenate((validation_sx[i], validation_rx[i]), axis=0))\n",
    "        validation_global_y = (np.concatenate((validation_sy[i], validation_ry[i]), axis=0))\n",
    "\n",
    "        clf.fit(training_global_x,training_global_y)\n",
    "\n",
    "        pred = (clf.predict(validation_global_x))\n",
    "        print(name, clf.score(validation_global_x, validation_global_y))\n",
    "\n",
    "        if (i == 3):\n",
    "            print(np.where((pred==validation_global_y)==False))\n",
    "        differencesB.append(difference(pred, validation_global_y) / len(validation_global_y))\n",
    "    print()\n",
    "\n",
    "# fold_error.append(cross_validation_error(5, splits))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for clf, name in models:\n",
    "    try:\n",
    "        validate(clf, name)\n",
    "    except:\n",
    "        print(\"\\t error training model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing test documents to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating test documents to run classificaiton on\n",
    "# vectors of document tokens\n",
    "test_vectorR = [w.lower() for w in reuters.words(reuters.fileids()[0])]\n",
    "test_vectorS = [w.lower() for w in SHAKESPEARE_WORDS[0]]\n",
    "test_mergedS =  [w.lower() for w in SHAKESPEARE_WORDS[1]]\n",
    "\n",
    "test_mergedS.extend(test_vectorS)\n",
    "test_mergedS = np.random.choice(test_mergedS, len(SHAKESPEARE_WORDS[1]))\n",
    "\n",
    "\n",
    "sparse_R = np.zeros(vocab.shape)\n",
    "sparse_S = np.zeros(vocab.shape)\n",
    "sparse_M = np.zeros(vocab.shape)\n",
    "\n",
    "# vectorizing array of tokens\n",
    "for w in test_vectorR:\n",
    "    i, = np.where(vocab == w)\n",
    "    sparse_R[i] = 1\n",
    "    \n",
    "for w in test_vectorS:\n",
    "    i, = np.where(vocab == w)\n",
    "    sparse_S[i] = 1\n",
    "    \n",
    "for w in test_mergedS:\n",
    "    i, = np.where(vocab == w)\n",
    "    sparse_M[i] = 1\n",
    "\n",
    "# sanity check\n",
    "print(sparse_M.shape)\n",
    "print(sparse_M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying the prepared test documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s = shakespeare_data\n",
    "X_r = reuters_data\n",
    "\n",
    "Y_s = np.zeros(X_s.shape[0])\n",
    "Y_r = np.ones(X_r.shape[0])\n",
    "\n",
    "X = np.concatenate((X_s, X_r), axis=0)\n",
    "Y = np.concatenate((Y_s, Y_r), axis=0)\n",
    "\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X,Y)\n",
    "\n",
    "pred = (clf.predict([sparse_M]))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Rate vs Threshold [work in progress]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_values = [100, 1000, 2000, 3000, 5000]\n",
    "\n",
    "# test_vector -> tokenized doc\n",
    "# vocab -> merged vocab\n",
    "def vectorize(test_vector,vocab):\n",
    "    sparse = np.zeros(vocab.shape)\n",
    "    for w in test_vector:\n",
    "        i, = np.where(vocab == w)\n",
    "        sparse[i] = 1\n",
    "    return sparse\n",
    "        \n",
    "        \n",
    "for threshold in threshold_values:\n",
    "    top_s= shakespeare_vocab_np[:threshold]\n",
    "    top_r = reuters_vocab_np[:threshold]\n",
    "    \n",
    "    intersect = np.intersect1d(top_s, top_r)\n",
    "    Vocab = np.concatenate((top_s, top_r), axis=0)\n",
    "    Vocab = np.array([v for v in vocab if v not in intersect])\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distribution of Corpera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_shake_50 = [w for w,f in topTuples_s[:51]]\n",
    "freq_shake_50 = [f for w,f in topTuples_s[:51]]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18.0, 7.0)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.bar(words_shake_50, freq_shake_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuters Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_reuters_50 = [w for w,f in topTuples_r[:51]]\n",
    "freq_reuters_50 = [f for w,f in topTuples_r[:51]]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18.0, 7.0)\n",
    "plt.xticks(rotation='vertical')\n",
    "bar_r = plt.bar(words_reuters_50, freq_reuters_50, color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE Corpus Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(raw_words: 'list of str'):\n",
    "    '''Filters raw word list then returns single string version'''\n",
    "    #convert to lowercase\n",
    "    lower = [word.lower() for word in raw_words]\n",
    "    \n",
    "    #remove punctuation from tokens\n",
    "    punc_filter = str.maketrans('', '', punctuation)\n",
    "    stripped = [word.translate(punc_filter) for word in lower]\n",
    "    \n",
    "    #remove remaining alphanumerics\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    #filter out basic stopwords\n",
    "    cleaned_word_bank = [word + ' ' for word in words if word not in stopset]\n",
    "    return ''.join(cleaned_word_bank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.text import TSNEVisualizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets.base import Bunch\n",
    "from yellowbrick.style import set_palette\n",
    "\n",
    "shake_folder = './Shake_works'\n",
    "textData_folder = './TextData'\n",
    "all_shake_files = [file for t,y, file in os.walk(shake_folder)][0]\n",
    "textData_dict = {}\n",
    "for _,folder,_ in os.walk(textData_folder):\n",
    "    for f in folder:\n",
    "        textData_dict[f] = [file for t,y,file in os.walk(textData_folder+'/'+f)][0]\n",
    "        \n",
    "punctuation = string.punctuation.replace(\"'\", \"\")\n",
    "stopset = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Load raw text data and labels simultaneously\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "subset = np.random.choice(reuters.fileids(), 158)\n",
    "# Reuters\n",
    "for file in subset:\n",
    "    raw_words = [str(word) for word in reuters.words(file)]\n",
    "    processed = process_words(raw_words)\n",
    "    data.append(processed)\n",
    "    labels.append('Reuters')\n",
    "    \n",
    "# Shakespeare\n",
    "for file in all_shake_files:\n",
    "    raw_words = [str(word) for word in open(shake_folder+'/'+file).read().split()]\n",
    "    processed = process_words(raw_words)\n",
    "    data.append(processed)\n",
    "    labels.append('Shakespeare')\n",
    "\n",
    "# TextData\n",
    "for time in textData_dict:\n",
    "    for file in textData_dict[time]:\n",
    "        print(time, file)\n",
    "        try:\n",
    "            raw_words = [str(word) for word in open(textData_folder+'/'+time+'/'+file, encoding='utf8').read().split()]\n",
    "        except:\n",
    "            continue\n",
    "        processed = process_words(raw_words)\n",
    "        data.append(processed)\n",
    "        labels.append(time)\n",
    "corpus = Bunch(\n",
    "    data=data,\n",
    "    target=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit tf-idf vectorizer\n",
    "TF_IDF = TfidfVectorizer()\n",
    "docs = TF_IDF.fit_transform(corpus.data)\n",
    "\n",
    "# Set color palette for plotting\n",
    "set_palette('sns_bright')\n",
    "\n",
    "# Construct visualizer and draw vectors\n",
    "tsne = TSNEVisualizer(colors=('r','b'))\n",
    "tsne.fit(docs, corpus.target)\n",
    "tsne.poof()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "print('---------------------\\nTruncated SVD Reuters\\n---------------------')\n",
    "\n",
    "SVD = TruncatedSVD(n_components=10, n_iter=30)\n",
    "SVD.fit_transform(reuters_data)\n",
    "\n",
    "print('Top 10 Reuters words account for {}% of overall variance'.format(round(SVD.explained_variance_ratio_.sum()*100, 2)))\n",
    "best_features = [vocab[i] for i in SVD.components_[0].argsort()[::-1]]\n",
    "\n",
    "i = 0\n",
    "for feature in best_features:\n",
    "    #if len(feature) >= 3:\n",
    "    print(feature)\n",
    "    i += 1\n",
    "    if i > 10:\n",
    "        break\n",
    "        \n",
    "print('\\n\\n')\n",
    "print('--------------------------\\nTruncated SVD Shakespeare\\n--------------------------')\n",
    "SVD = TruncatedSVD(n_components=10, n_iter=30)\n",
    "SVD.fit_transform(shakespeare_data)\n",
    "\n",
    "print('Top 10 Shakespeare words account for {}% of overall variance'.format(round(SVD.explained_variance_ratio_.sum()*100, 2)))\n",
    "best_features = [vocab[i] for i in SVD.components_[0].argsort()[::-1]]\n",
    "\n",
    "i = 0\n",
    "for feature in best_features:\n",
    "    print(feature)\n",
    "    i += 1\n",
    "    if i > 10:\n",
    "        break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf, name in models:\n",
    "    clf.fit(X,y)\n",
    "\n",
    "def predict(clf, model, doc):\n",
    "    print(\"Prediction for model: \" + model)\n",
    "    print(clf.predict([doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def runCentury(century, clf, desc):\n",
    "    print(\"Classification of documents from the \" + century)\n",
    "\n",
    "    onlyfiles = [f for f in listdir(\"TextData/\"+century) if isfile(join(\"TextData/\"+century, f))]\n",
    "\n",
    "    for f in onlyfiles:\n",
    "        doc = []\n",
    "        try:\n",
    "            with open(join(\"TextData/\"+century,f), \"r\", encoding=\"utf-8-sig\") as fi:\n",
    "                word_list = [w.lower() for w in word_tokenize(fi.read())]\n",
    "                doc = vectorize(word_list, vocab)\n",
    "            predict(clf, desc, doc)\n",
    "        except(UnicodeDecodeError):\n",
    "            with open(join(\"TextData/\"+century,f), \"r\") as fi:\n",
    "                word_list = [w.lower() for w in word_tokenize(fi.read())]\n",
    "                doc = vectorize(word_list, vocab)\n",
    "            predict(clf, desc, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "centuries = [\"1600s\",\"1700s\",\"1800s\",\"1900s\",\"2000s\"]\n",
    "for c in centuries:\n",
    "    for clf in models:\n",
    "        runCentury(c,clf[0],clf[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "# run validation cell beforehand, this function needs the splits\n",
    "def gen_confusion(model, modelName = \"\"):\n",
    "    fig, ax = plt.subplots(1, 5)\n",
    "    for i in range(len(training_sx)):\n",
    "            print(i)\n",
    "            plt.rcParams['figure.figsize']=(18.0,12.0)\n",
    "            ax[i].set_title(modelName)\n",
    "            cm = ConfusionMatrix(ax=ax[i], model=model, classes=[0,1])\n",
    "\n",
    "            training_global_x = (np.concatenate((training_sx[i], training_rx[i]), axis=0))\n",
    "            training_global_y = (np.concatenate((training_sy[i], training_ry[i]), axis=0))\n",
    "            validation_global_x = (np.concatenate((validation_sx[i], validation_rx[i]), axis=0))\n",
    "            validation_global_y = (np.concatenate((validation_sy[i], validation_ry[i]), axis=0))\n",
    "\n",
    "            cm.fit(training_global_x,training_global_y)\n",
    "            cm.score(validation_global_x, validation_global_y)\n",
    "\n",
    "            \n",
    "            \n",
    "modelNum = 0\n",
    "fig, ax = plt.subplots(3, 5)\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "plt.rcParams['figure.figsize']=(18.0,12.0)  \n",
    "modelDict = {\"Bernoulli Naive Bayes\":clfBernoulli, \"Gaussian Naive Bayes\":clfGaussian, \"Logistic Regression\":clfLR}\n",
    "\n",
    "for modelName, model in modelsDict.():\n",
    "        for i in range(len(training_sx)):\n",
    "            print(i)\n",
    "            plt.rcParams['figure.figsize']=(18.0,12.0)\n",
    "            ax[modelNum, i].set_title(modelName)\n",
    "            cm = ConfusionMatrix(ax=ax[modelNum, i], model=model, classes=[0,1])\n",
    "\n",
    "            training_global_x = (np.concatenate((training_sx[i], training_rx[i]), axis=0))\n",
    "            training_global_y = (np.concatenate((training_sy[i], training_ry[i]), axis=0))\n",
    "            validation_global_x = (np.concatenate((validation_sx[i], validation_rx[i]), axis=0))\n",
    "            validation_global_y = (np.concatenate((validation_sy[i], validation_ry[i]), axis=0))\n",
    "\n",
    "            cm.fit(training_global_x,training_global_y)\n",
    "            cm.score(validation_global_x, validation_global_y)\n",
    "        modelNum += 1\n",
    "\n",
    "# cm = ConfusionMatrix(ax=ax[modelNum, i], model=clfS, classes=[0,1])\n",
    "# cm.fit(training_global_x,training_global_y)\n",
    "# cm.score(validation_global_x, validation_global_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
