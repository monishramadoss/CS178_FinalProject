{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent import futures\n",
    "import nltk\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from scipy import sparse\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "NUM_THREADS = 24\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "reuters_freqThreshold = 5000\n",
    "shakespeare_freqThreshold = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuters Data Scrubbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(reuters.fileids()))\n",
    "punctuation = string.punctuation.replace(\"'\", \"\")\n",
    "stopset = set(stopwords.words(\"english\"))\n",
    "\n",
    "def thread_task(rawWords):\n",
    "    freqMap = defaultdict(int)\n",
    "    #convert to lowercase\n",
    "    lower = [word.lower() for word in rawWords]\n",
    "    \n",
    "    #remove punctuation from tokens\n",
    "    punc_filter = str.maketrans('', '', punctuation)\n",
    "    stripped = [word.translate(punc_filter) for word in lower]\n",
    "    \n",
    "    #remove remaining alphanumerics\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    #filter out basic stopwords\n",
    "    cleaned_word_bank = [word for word in words if word not in stopset]\n",
    "    \n",
    "    for word in cleaned_word_bank:\n",
    "        freqMap[word] += 1\n",
    "    return freqMap\n",
    "\n",
    "def thread_exec(WORDS):\n",
    "    with futures.ThreadPoolExecutor(max_workers=NUM_THREADS) as ex:\n",
    "        results = list(ex.map(thread_task, WORDS))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqMap = defaultdict(int)\n",
    "REUTERS_WORDS = [[str(word) for word in reuters.words(file)] for file in reuters.fileids()][0:5]\n",
    "t = thread_exec(REUTERS_WORDS)\n",
    "for ifreqMap in t:\n",
    "    for k in ifreqMap.keys():    \n",
    "        freqMap[k] += ifreqMap[k]    \n",
    "        \n",
    "freqTuples = list(freqMap.items())\n",
    "sorted_freq = freqTuples.sort(key= lambda x: x[1], reverse=True)\n",
    "topTuples_r = freqTuples[:reuters_freqThreshold]\n",
    "filtered_vocab = [tup[0] for tup in topTuples_r]\n",
    "reuters_vocab_np = np.asarray(filtered_vocab, dtype='str')\n",
    "print(reuters_vocab_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shakespeare Data Scrubbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './Shake_works'\n",
    "sub = \"[^a-zA-Z' ]+\"\n",
    "all_files = [file for t,y, file in os.walk(folder)][0]\n",
    "stopset = set(stopwords.words(\"english\"))\n",
    "\n",
    "def thread_task(rawWords):\n",
    "    freqMap = defaultdict(int)\n",
    "    words = [re.sub(sub, '', word) for word in rawWords]\n",
    "    lower = [word.lower() for word in words]\n",
    "    words = [word for word in lower if word.isalpha()]\n",
    "    cleaned_word_bank = [word for word in words if word not in stopset]    \n",
    "    \n",
    "    for word in cleaned_word_bank:\n",
    "        freqMap[word] += 1\n",
    "    return freqMap\n",
    "\n",
    "def thread_exec(WORDS):\n",
    "    with futures.ThreadPoolExecutor(max_workers=NUM_THREADS) as ex:\n",
    "        results = list(ex.map(thread_task, WORDS))\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqMap = defaultdict(int)\n",
    "SHAKESPEARE_WORDS = [[str(word) for word in open(folder+'/'+file).read().split()] for file in all_files]\n",
    "t = thread_exec(SHAKESPEARE_WORDS)\n",
    "for ifreqMap in t:\n",
    "    for k in ifreqMap.keys():    \n",
    "        freqMap[k] += ifreqMap[k]   \n",
    "        \n",
    "freqTuples = list(freqMap.items())\n",
    "sorted_freq = freqTuples.sort(key= lambda x: x[1], reverse=True)\n",
    "topTuples_s = freqTuples[:shakespeare_freqThreshold]\n",
    "filtered_vocab = [tup[0] for tup in topTuples_s]\n",
    "shakespeare_vocab_np = np.asarray(filtered_vocab, dtype='str')\n",
    "print(shakespeare_vocab_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputting top words/frequencies to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_freq_file = open(\"reuters_frequencies.txt\", \"w+\")\n",
    "shakes_freq_file = open(\"shakes_frequencies.txt\", \"w+\")\n",
    "\n",
    "for tup in topTuples_r:\n",
    "    reuters_freq_file.write(f\"{tup[0]}, {tup[1]}\")\n",
    "for tup in topTuples_s:\n",
    "    shakes_freq_file.write(f\"{tup[0]}, {tup[1]}\")\n",
    "    \n",
    "reuters_freq_file.close()\n",
    "shakes_freq_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress, HTML, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "vocab = reuters_vocab_np\n",
    "tmp = list()\n",
    "for word in shakespeare_vocab_np:\n",
    "    if(word not in vocab):\n",
    "        tmp.append(word)\n",
    "vocab = np.append(vocab, tmp)\n",
    "print(vocab.shape)\n",
    "\n",
    "\n",
    "shakespeare_data = []\n",
    "i=0\n",
    "punc_filter = str.maketrans('', '', punctuation)\n",
    "progress = IntProgress(min=0, max=len(SHAKESPEARE_WORDS))\n",
    "label = HTML()\n",
    "box = VBox(children=[label, progress])\n",
    "display(box)\n",
    "for doc in SHAKESPEARE_WORDS:\n",
    "    shakespeare_data.append(np.zeros(vocab.shape))\n",
    "    #print(doc)\n",
    "    for word in doc:\n",
    "        word = re.sub(sub,'',word).lower()\n",
    "        shakespeare_data[i][vocab==word] = 1.0\n",
    "    i+=1\n",
    "    progress.value += 1\n",
    "    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=\"Docs\",\n",
    "                        index=i,\n",
    "                        size=len(SHAKESPEARE_WORDS)\n",
    "                    )\n",
    "shakespeare_data = np.array(shakespeare_data)   \n",
    "\n",
    "\n",
    "reuters_data = []\n",
    "i=0\n",
    "punc_filter = str.maketrans('', '', punctuation)\n",
    "progress = IntProgress(min=0, max=len(REUTERS_WORDS))\n",
    "label = HTML()\n",
    "box = VBox(children=[label, progress])\n",
    "display(box)\n",
    "for doc in REUTERS_WORDS:\n",
    "    reuters_data.append(np.zeros(vocab.shape))\n",
    "    for word in doc:\n",
    "        word = word.lower()\n",
    "        word = word.translate(punc_filter)\n",
    "        reuters_data[i][vocab==word] = 1.0\n",
    "    i+=1   \n",
    "    progress.value += 1\n",
    "    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=\"Docs\",\n",
    "                        index=i,\n",
    "                        size=len(REUTERS_WORDS)\n",
    "                    )\n",
    "reuters_data = np.array(reuters_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = shakespeare_data\n",
    "y = np.zeros(X.shape[0])\n",
    "\n",
    "X = np.concatenate((X, reuters_data), axis=0)\n",
    "y = np.concatenate((y, np.ones(reuters_data.shape[0])), axis=0)\n",
    "\n",
    "vocab = np.array(vocab, dtype=np.str)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(vocab.shape)\n",
    "\n",
    "X.dump('X')\n",
    "y.dump('y')\n",
    "vocab.dump('vocab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('X')\n",
    "y = np.load('y')\n",
    "vocab = np.load('vocab')\n",
    "Model_List = list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(l1,l2):\n",
    "    precisionError = len([b for a,b in zip(l1,l2) if b != a and b == 0 and a == 1])\n",
    "    recallError = len([b for a,b in zip(l1,l2) if b != a and b == 1 and a == 0])\n",
    "    print(\"\\tPrecision Error is \" + str(precisionError))\n",
    "    print(\"\\tRecall Error is \" + str(recallError))\n",
    "\n",
    "    return (precisionError) + (recallError)\n",
    "\n",
    "def runModels(clf, Xtr, Ytr):\n",
    "    kf = KFold(n_splits=5)    \n",
    "    differencesB = []\n",
    "    differencesG = []\n",
    "\n",
    "    for train_index, test_index in kf.split(Xtr):\n",
    "        x_train, x_test = [Xtr[i] for i in train_index], [Xtr[i] for i in test_index]\n",
    "        y_train, y_test = [Ytr[i] for i in train_index], [Ytr[i] for i in test_index]    \n",
    "        clf.fit(x_train,y_train)\n",
    "        yhat = clf.predict(x_test)\n",
    "        mse = ((y_test-yhat)**2).mean(axis = 0)\n",
    "        print(\"\\tMSE: \", mse)\n",
    "        Model_List.append(clf)\n",
    "        differencesB.append(difference(clf.predict(x_test), y_test) / len(y_test))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bernoulli Naive Bayes\")\n",
    "runModels(BernoulliNB(), X, y)\n",
    "print(\"Gaussian Naive Bayes\")\n",
    "runModels(GaussianNB(), X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fold_error = []\n",
    "print(\"seperating start\")\n",
    "X_s = X[y==0]\n",
    "X_r = X[y==1]\n",
    "\n",
    "Y_s = np.zeros(X_s.shape[0])\n",
    "Y_r = np.ones(X_r.shape[0])\n",
    "\n",
    "print(\"seperated docs\")\n",
    "\n",
    "## 5 -> 8 shakespere docs\n",
    "## 5 -> ~2k reuters docs\n",
    "folds = KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "splits = []\n",
    "training_sx = []\n",
    "training_sy = []\n",
    "\n",
    "validation_sx = []\n",
    "validation_sy = []\n",
    "\n",
    "\n",
    "training_rx = []\n",
    "training_ry = []\n",
    "validation_rx = []\n",
    "validation_ry = []\n",
    "\n",
    "differencesB = []\n",
    "\n",
    "\n",
    "print(\"splitting shakespere\")\n",
    "for train_index, test_index in folds.split(X_s, Y_s):\n",
    "    x_tr, x_validation = X_s[train_index], X_s[test_index]\n",
    "    y_tr, y_validation = Y_s[train_index], Y_s[test_index]\n",
    "    training_sx.append(x_tr)\n",
    "    training_sy.append(y_tr)\n",
    "    validation_sx.append(x_validation) \n",
    "    validation_sy.append(y_validation)\n",
    "print(\"split shakespeare done\")\n",
    "    \n",
    "for train_index, test_index in folds.split(X_r, Y_r):\n",
    "    x_tr, x_validation = X_r[train_index], X_r[test_index]\n",
    "    y_tr, y_validation = Y_r[train_index], Y_r[test_index]\n",
    "    training_rx.append(x_tr)\n",
    "    training_ry.append(y_tr)\n",
    "    validation_rx.append(x_validation) \n",
    "    validation_ry.append(y_validation)\n",
    "print(\"split shakespere done\")\n",
    "\n",
    "print(\"training\")\n",
    "\n",
    "print(training_rx[0])\n",
    "def validate(clf, name):\n",
    "    for i in range(len(training_sx)):\n",
    "        training_global_x = (np.concatenate((training_sx[i], training_rx[i]), axis=0))\n",
    "        training_global_y = (np.concatenate((training_sy[i], training_ry[i]), axis=0))\n",
    "        validation_global_x = (np.concatenate((validation_sx[i], validation_rx[i]), axis=0))\n",
    "        validation_global_y = (np.concatenate((validation_sy[i], validation_ry[i]), axis=0))\n",
    "\n",
    "        clf.fit(training_global_x,training_global_y)\n",
    "\n",
    "        pred = (clf.predict(validation_global_x))\n",
    "        print(name, clf.score(validation_global_x, validation_global_y))\n",
    "\n",
    "        if (i == 3):\n",
    "            print(np.where((pred==validation_global_y)==False))\n",
    "        differencesB.append(difference(pred, validation_global_y) / len(validation_global_y))\n",
    "    print()\n",
    "\n",
    "# fold_error.append(cross_validation_error(5, splits))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validate(BernoulliNB(), 'Naive Bayes')\n",
    "validate(GaussianNB(), 'G Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing test documents to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating test documents to run classificaiton on\n",
    "all_files = reuters.fileids()\n",
    "\n",
    "\n",
    "# vectors of document tokens\n",
    "test_vectorR = [w.lower() for w in reuters.words(all_files[0])]\n",
    "\n",
    "test_vectorS = [w.lower() for w in SHAKESPEARE_WORDS[0]]\n",
    "\n",
    "test_mergedS =  [w.lower() for w in SHAKESPEARE_WORDS[1]]\n",
    "\n",
    "test_mergedS.extend(test_vectorS)\n",
    "\n",
    "test_mergedS = np.random.choice(test_mergedS, len(SHAKESPEARE_WORDS[1]))\n",
    "\n",
    "\n",
    "sparse_R = np.zeros(vocab.shape)\n",
    "sparse_S = np.zeros(vocab.shape)\n",
    "sparse_M = np.zeros(vocab.shape)\n",
    "\n",
    "# vectorizing array of tokens\n",
    "for w in test_vectorR:\n",
    "    i, = np.where(vocab == w)\n",
    "    sparse_R[i] = 1\n",
    "    \n",
    "for w in test_vectorS:\n",
    "    i, = np.where(vocab == w)\n",
    "    sparse_S[i] = 1\n",
    "    \n",
    "for w in test_mergedS:\n",
    "    i, = np.where(vocab == w)\n",
    "    sparse_M[i] = 1\n",
    "\n",
    "# sanity check\n",
    "print(sparse_M.shape)\n",
    "print(sparse_M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying the prepared test documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s = shakespeare_data\n",
    "X_r = reuters_data\n",
    "\n",
    "Y_s = np.zeros(X_s.shape[0])\n",
    "Y_r = np.ones(X_r.shape[0])\n",
    "\n",
    "X = np.concatenate((X_s, X_r), axis=0)\n",
    "Y = np.concatenate((Y_s, Y_r), axis=0)\n",
    "\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X,Y)\n",
    "\n",
    "pred = (clf.predict([sparse_M]))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Rate vs Threshold [work in progress]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_values = [100, 1000, 2000, 3000, 5000]\n",
    "\n",
    "# test_vector -> tokenized doc\n",
    "# vocab -> merged vocab\n",
    "def vectorize(test_vector,vocab):\n",
    "    sparse = np.zeros(vocab.shape)\n",
    "    for w in test_vector:\n",
    "        i, = np.where(vocab == w)\n",
    "        sparse[i] = 1\n",
    "    return sparse\n",
    "        \n",
    "        \n",
    "for threshold in threshold_values:\n",
    "    top_s= shakespeare_vocab_np[:threshold]\n",
    "    top_r = reuters_vocab_np[:threshold]\n",
    "    \n",
    "    intersect = np.intersect1d(top_s, top_r)\n",
    "    Vocab = np.concatenate((top_s, top_r), axis=0)\n",
    "    Vocab = np.array([v for v in vocab if v not in intersect])\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distribution of Corpera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_shake_50 = [w for w,f in topTuples_s[:51]]\n",
    "freq_shake_50 = [f for w,f in topTuples_s[:51]]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18.0, 7.0)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.bar(words_shake_50, freq_shake_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuters Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_reuters_50 = [w for w,f in topTuples_r[:51]]\n",
    "freq_reuters_50 = [f for w,f in topTuples_r[:51]]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18.0, 7.0)\n",
    "plt.xticks(rotation='vertical')\n",
    "bar_r = plt.bar(words_reuters_50, freq_reuters_50, color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE Corpus Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(raw_words: 'list of str'):\n",
    "    '''Filters raw word list then returns single string version'''\n",
    "    #convert to lowercase\n",
    "    lower = [word.lower() for word in raw_words]\n",
    "    \n",
    "    #remove punctuation from tokens\n",
    "    punc_filter = str.maketrans('', '', punctuation)\n",
    "    stripped = [word.translate(punc_filter) for word in lower]\n",
    "    \n",
    "    #remove remaining alphanumerics\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    #filter out basic stopwords\n",
    "    cleaned_word_bank = [word + ' ' for word in words if word not in stopset]\n",
    "    return ''.join(cleaned_word_bank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.text import TSNEVisualizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets.base import Bunch\n",
    "from yellowbrick.style import set_palette\n",
    "\n",
    "shake_folder = './Shake_works'\n",
    "textData_folder = './TextData'\n",
    "all_shake_files = [file for t,y, file in os.walk(shake_folder)][0]\n",
    "textData_dict = {}\n",
    "for _,folder,_ in os.walk(textData_folder):\n",
    "    for f in folder:\n",
    "        textData_dict[f] = [file for t,y,file in os.walk(textData_folder+'/'+f)][0]\n",
    "        \n",
    "punctuation = string.punctuation.replace(\"'\", \"\")\n",
    "stopset = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Load raw text data and labels simultaneously\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "subset = np.random.choice(reuters.fileids(), 158)\n",
    "# Reuters\n",
    "for file in subset:\n",
    "    raw_words = [str(word) for word in reuters.words(file)]\n",
    "    processed = process_words(raw_words)\n",
    "    data.append(processed)\n",
    "    labels.append('Reuters')\n",
    "    \n",
    "# Shakespeare\n",
    "for file in all_shake_files:\n",
    "    raw_words = [str(word) for word in open(shake_folder+'/'+file).read().split()]\n",
    "    processed = process_words(raw_words)\n",
    "    data.append(processed)\n",
    "    labels.append('Shakespeare')\n",
    "\n",
    "# TextData\n",
    "for time in textData_dict:\n",
    "    for file in textData_dict[time]:\n",
    "        print(time, file)\n",
    "        try:\n",
    "            raw_words = [str(word) for word in open(textData_folder+'/'+time+'/'+file, encoding='utf8').read().split()]\n",
    "        except:\n",
    "            continue\n",
    "        processed = process_words(raw_words)\n",
    "        data.append(processed)\n",
    "        labels.append(time)\n",
    "corpus = Bunch(\n",
    "    data=data,\n",
    "    target=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit tf-idf vectorizer\n",
    "TF_IDF = TfidfVectorizer()\n",
    "docs = TF_IDF.fit_transform(corpus.data)\n",
    "\n",
    "# Set color palette for plotting\n",
    "set_palette('sns_bright')\n",
    "\n",
    "# Construct visualizer and draw vectors\n",
    "tsne = TSNEVisualizer(colors=('r','b'))\n",
    "tsne.fit(docs, corpus.target)\n",
    "tsne.poof()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "print('---------------------\\nTruncated SVD Reuters\\n---------------------')\n",
    "\n",
    "SVD = TruncatedSVD(n_components=10, n_iter=30)\n",
    "SVD.fit_transform(reuters_data)\n",
    "\n",
    "print('Top 10 Reuters words account for {}% of overall variance'.format(round(SVD.explained_variance_ratio_.sum()*100, 2)))\n",
    "best_features = [vocab[i] for i in SVD.components_[0].argsort()[::-1]]\n",
    "\n",
    "i = 0\n",
    "for feature in best_features:\n",
    "    #if len(feature) >= 3:\n",
    "    print(feature)\n",
    "    i += 1\n",
    "    if i > 10:\n",
    "        break\n",
    "        \n",
    "print('\\n\\n')\n",
    "print('--------------------------\\nTruncated SVD Shakespeare\\n--------------------------')\n",
    "SVD = TruncatedSVD(n_components=10, n_iter=30)\n",
    "SVD.fit_transform(shakespeare_data)\n",
    "\n",
    "print('Top 10 Shakespeare words account for {}% of overall variance'.format(round(SVD.explained_variance_ratio_.sum()*100, 2)))\n",
    "best_features = [vocab[i] for i in SVD.components_[0].argsort()[::-1]]\n",
    "\n",
    "i = 0\n",
    "for feature in best_features:\n",
    "    print(feature)\n",
    "    i += 1\n",
    "    if i > 10:\n",
    "        break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(clf, model, doc):\n",
    "    print(\"Prediction for model: \" + model)\n",
    "    print(clf.predict([doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "def runCentury(century, clf, desc):\n",
    "    print(\"Classification of documents from the \" + century)\n",
    "\n",
    "    onlyfiles = [f for f in listdir(\"TextData/\"+century) if isfile(join(\"TextData/\"+century, f))]\n",
    "\n",
    "    for f in onlyfiles:\n",
    "        doc = []\n",
    "        with open(filename, encoding=\"utf8\") as f:\n",
    "            doc = vectorize(f, vocab)\n",
    "\n",
    "        predict(clf, desc, doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Microsoft\\VisualStudio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Program Files\\Microsoft\\VisualStudio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clfBernoulli = BernoulliNB()\n",
    "clfBernoulli.fit(X,y)\n",
    "\n",
    "clfGaussian = GaussianNB()\n",
    "clfGaussian.fit(X,y)\n",
    "\n",
    "clfLR = LogisticRegression()\n",
    "clfLR.fit(X,y)\n",
    "\n",
    "clfKnn = KNeighborsClassifier(n_neighbors=5)\n",
    "clfKnn.fit(X,y)\n",
    "\n",
    "clfSVM = svm.SVC()\n",
    "clfSVM.fit(X,y)\n",
    "\n",
    "models = [(clfBernoulli, \"Bernoulli Naive Bayes\"), (clfGaussian, \"Gaussian Naive Bayes\"), (clfLR, \"Logistic Regression\"), (clfKnn, \"K Nearest Neighbours\"), (clfSVM, \"Support Vector Machine\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification of documents from the 1600s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-a3c4d321f4a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcenturies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mrunCentury\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-49-3fb713722640>\u001b[0m in \u001b[0;36mrunCentury\u001b[1;34m(century, clf, desc)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0monlyfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filename' is not defined"
     ]
    }
   ],
   "source": [
    "centuries = [\"1600s\",\"1700s\",\"1800s\",\"1900s\",\"2000s\"]\n",
    "for c in centuries:\n",
    "    for clf in models:\n",
    "        runCentury(c,clf[0],clf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
