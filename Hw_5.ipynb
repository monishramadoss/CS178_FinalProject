{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\bgalk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bgalk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bgalk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from concurrent import futures\n",
    "import nltk\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from scipy import sparse\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "NUM_THREADS = 24\n",
    "\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "reuters_freqThreshold = 5000\n",
    "shakespeare_freqThreshold = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuters Data Scrubbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10788\n"
     ]
    }
   ],
   "source": [
    "all_files = reuters.fileids()\n",
    "file_count = len(all_files)\n",
    "print(len(reuters.fileids()))\n",
    "punctuation = string.punctuation.replace(\"'\", \"\")\n",
    "stopset = set(stopwords.words(\"english\"))\n",
    "\n",
    "def thread_task(rawWords):\n",
    "    freqMap = defaultdict(int)\n",
    "    #convert to lowercase\n",
    "    lower = [word.lower() for word in rawWords]\n",
    "    \n",
    "    #remove punctuation from tokens\n",
    "    punc_filter = str.maketrans('', '', punctuation)\n",
    "    stripped = [word.translate(punc_filter) for word in lower]\n",
    "    \n",
    "    #remove remaining alphanumerics\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    #filter out basic stopwords\n",
    "    cleaned_word_bank = [word for word in words if word not in stopset]\n",
    "    \n",
    "    for word in cleaned_word_bank:\n",
    "        freqMap[word] += 1\n",
    "    return freqMap\n",
    "\n",
    "def thread_exec(WORDS):\n",
    "    with futures.ThreadPoolExecutor(max_workers=NUM_THREADS) as ex:\n",
    "        results = list(ex.map(thread_task, WORDS))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "freqMap = defaultdict(int)\n",
    "REUTERS_WORDS = [[str(word) for word in reuters.words(file)] for file in reuters.fileids()]\n",
    "t = thread_exec(REUTERS_WORDS)\n",
    "for ifreqMap in t:\n",
    "    for k in ifreqMap.keys():    \n",
    "        freqMap[k] += ifreqMap[k]    \n",
    "        \n",
    "freqTuples = list(freqMap.items())\n",
    "sorted_freq = freqTuples.sort(key= lambda x: x[1], reverse=True)\n",
    "topTuples_r = freqTuples[:reuters_freqThreshold]\n",
    "filtered_vocab = [tup[0] for tup in topTuples_r]\n",
    "reuters_vocab_np = np.asarray(filtered_vocab, dtype='str')\n",
    "print(reuters_vocab_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shakespeare Data Scrubbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './works'\n",
    "sub = \"[^a-zA-Z' ]+\"\n",
    "all_files = [file for t,y, file in os.walk(folder)][0]\n",
    "stopset = set(stopwords.words(\"english\"))\n",
    "\n",
    "def thread_task(rawWords):\n",
    "    freqMap = defaultdict(int)\n",
    "    words = [re.sub(sub, '', word) for word in rawWords]\n",
    "    lower = [word.lower() for word in words]\n",
    "    words = [word for word in lower if word.isalpha()]\n",
    "    cleaned_word_bank = [word for word in words if word not in stopset]    \n",
    "    \n",
    "    for word in cleaned_word_bank:\n",
    "        freqMap[word] += 1\n",
    "    return freqMap\n",
    "\n",
    "def thread_exec(WORDS):\n",
    "    with futures.ThreadPoolExecutor(max_workers=NUM_THREADS) as ex:\n",
    "        results = list(ex.map(thread_task, WORDS))\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "freqMap = defaultdict(int)\n",
    "SHAKESPEARE_WORDS = [[str(word) for word in open(folder+'/'+file).read().split()] for file in all_files]\n",
    "t = thread_exec(SHAKESPEARE_WORDS)\n",
    "for ifreqMap in t:\n",
    "    for k in ifreqMap.keys():    \n",
    "        freqMap[k] += ifreqMap[k]   \n",
    "        \n",
    "freqTuples = list(freqMap.items())\n",
    "sorted_freq = freqTuples.sort(key= lambda x: x[1], reverse=True)\n",
    "topTuples_s = freqTuples[:shakespeare_freqThreshold]\n",
    "filtered_vocab = [tup[0] for tup in topTuples_s]\n",
    "shakespeare_vocab_np = np.asarray(filtered_vocab, dtype='str')\n",
    "print(shakespeare_vocab_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputting top words/frequencies to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_freq_file = open(\"reuters_frequencies.txt\", \"w+\")\n",
    "shakes_freq_file = open(\"shakes_frequencies.txt\", \"w+\")\n",
    "\n",
    "for tup in topTuples_r:\n",
    "    reuters_freq_file.write(f\"{tup[0]}, {tup[1]}\")\n",
    "for tup in topTuples_s:\n",
    "    shakes_freq_file.write(f\"{tup[0]}, {tup[1]}\")\n",
    "    \n",
    "reuters_freq_file.close()\n",
    "shakes_freq_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8586,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7dfda4dfeb491e987b349af67b8c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=42)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa9f305124e4f3d83d25a7eaa1dae0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=10788)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import IntProgress, HTML, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "vocab = reuters_vocab_np\n",
    "tmp = list()\n",
    "for word in shakespeare_vocab_np:\n",
    "    if(word not in vocab):\n",
    "        tmp.append(word)\n",
    "vocab = np.append(vocab, tmp)\n",
    "print(vocab.shape)\n",
    "\n",
    "\n",
    "shakespeare_data = []\n",
    "i=0\n",
    "punc_filter = str.maketrans('', '', punctuation)\n",
    "progress = IntProgress(min=0, max=len(SHAKESPEARE_WORDS))\n",
    "label = HTML()\n",
    "box = VBox(children=[label, progress])\n",
    "display(box)\n",
    "for doc in SHAKESPEARE_WORDS:\n",
    "    shakespeare_data.append(np.zeros(vocab.shape))\n",
    "    #print(doc)\n",
    "    for word in doc:\n",
    "        word = re.sub(sub,'',word).lower()\n",
    "        shakespeare_data[i][vocab==word] = 1.0\n",
    "    i+=1\n",
    "    progress.value += 1\n",
    "    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=\"Docs\",\n",
    "                        index=i,\n",
    "                        size=len(SHAKESPEARE_WORDS)\n",
    "                    )\n",
    "shakespeare_data = np.array(shakespeare_data)   \n",
    "\n",
    "\n",
    "reuters_data = []\n",
    "i=0\n",
    "punc_filter = str.maketrans('', '', punctuation)\n",
    "progress = IntProgress(min=0, max=len(REUTERS_WORDS))\n",
    "label = HTML()\n",
    "box = VBox(children=[label, progress])\n",
    "display(box)\n",
    "for doc in REUTERS_WORDS:\n",
    "    reuters_data.append(np.zeros(vocab.shape))\n",
    "    for word in doc:\n",
    "        word = word.lower()\n",
    "        word = word.translate(punc_filter)\n",
    "        reuters_data[i][vocab==word] = 1.0\n",
    "    i+=1   \n",
    "    progress.value += 1\n",
    "    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=\"Docs\",\n",
    "                        index=i,\n",
    "                        size=len(REUTERS_WORDS)\n",
    "                    )\n",
    "reuters_data = np.array(reuters_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10830, 8586)\n",
      "(10830,)\n"
     ]
    }
   ],
   "source": [
    "X = shakespeare_data\n",
    "y = np.zeros(X.shape[0])\n",
    "\n",
    "X = np.concatenate((X, reuters_data), axis=0)\n",
    "y = np.concatenate((y, np.ones(reuters_data.shape[0])), axis=0)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# X.dump('X')\n",
    "# y.dump('y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('X')\n",
    "y = np.load('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(l1,l2):\n",
    "    precisionError = len([b for a,b in zip(l1,l2) if b != a and b == 0 and a == 1])\n",
    "    recallError = len([b for a,b in zip(l1,l2) if b != a and b == 1 and a == 0])\n",
    "    print(\"Precision Error is \" + str(precisionError))\n",
    "    print(\"Recall Error is \" + str(recallError))\n",
    "\n",
    "    return (precisionError) + (recallError)\n",
    "\n",
    "def runModels(Xtr, Ytr):\n",
    "    kf = KFold(n_splits=5)    \n",
    "    differencesB = []\n",
    "    differencesG = []\n",
    "\n",
    "    for train_index, test_index in kf.split(Xtr):\n",
    "        x_train, x_test = [Xtr[i] for i in train_index], [Xtr[i] for i in test_index]\n",
    "        y_train, y_test = [Ytr[i] for i in train_index], [Ytr[i] for i in test_index]\n",
    "        \n",
    "        clf = BernoulliNB()\n",
    "        clf.fit(x_train,y_train)\n",
    "        \n",
    "        print(clf.predict(x_test))\n",
    "        differencesB.append(difference(clf.predict(x_test), y_test) / len(y_test))\n",
    "\n",
    "        clfb = GaussianNB()\n",
    "        clfb.fit(x_train,y_train)\n",
    "        \n",
    "        print(clfb.predict(x_test))\n",
    "        differencesG.append(difference(clfb.predict(x_test), y_test) / len(y_test))\n",
    "\n",
    "    return differencesB, differencesG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "([0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0])\n"
     ]
    }
   ],
   "source": [
    "print(runModels(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom k-fold cross validation code\n",
    "# custom k-fold cross validation code\n",
    "\n",
    "# def k_fold(X, Y1, Y2,folds)\n",
    "#     split_size = X.shape[0]//folds\n",
    "#     splits = []\n",
    "#     for i in range(folds):\n",
    "#         start = split_size*i\n",
    "#         end = (split_size*(i+1))\n",
    "#         validation = (X[start:end,], Y[start:end])\n",
    "#         train_x = np.append(X[:start,], X[end:,])\n",
    "#         train_y = np.append(Y[:start],Y[end:])\n",
    "#         training = (np.atleast_2d(train_x).T, train_y)\n",
    "#         splits.append((training, validation))\n",
    "#     return splits\n",
    "\n",
    "fold_error = []\n",
    "print(\"seperating start\")\n",
    "X_s = shakespeare_data\n",
    "X_r = reuters_data\n",
    "\n",
    "Y_s = np.zeros(X_s.shape[0])\n",
    "Y_r = np.ones(X_r.shape[0])\n",
    "\n",
    "print(\"seperated docs\")\n",
    "\n",
    "## 5 -> 8 shakespere docs\n",
    "## 5 -> ~2k reuters docs\n",
    "folds = KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "splits = []\n",
    "training_sx = []\n",
    "training_sy = []\n",
    "\n",
    "validation_sx = []\n",
    "validation_sy = []\n",
    "\n",
    "\n",
    "training_rx = []\n",
    "training_ry = []\n",
    "validation_rx = []\n",
    "validation_ry = []\n",
    "\n",
    "differencesB = []\n",
    "\n",
    "\n",
    "print(\"splitting shakespere\")\n",
    "for train_index, test_index in folds.split(X_s, Y_s):\n",
    "    x_tr, x_validation = X_s[train_index], X_s[test_index]\n",
    "    y_tr, y_validation = Y_s[train_index], Y_s[test_index]\n",
    "    training_sx.append(x_tr)\n",
    "    training_sy.append(y_tr)\n",
    "    validation_sx.append(x_validation) \n",
    "    validation_sy.append(y_validation)\n",
    "print(\"split shakespere done\")\n",
    "    \n",
    "for train_index, test_index in folds.split(X_r, Y_r):\n",
    "    x_tr, x_validation = X_r[train_index], X_r[test_index]\n",
    "    y_tr, y_validation = Y_r[train_index], Y_r[test_index]\n",
    "    training_rx.append(x_tr)\n",
    "    training_ry.append(y_tr)\n",
    "    validation_rx.append(x_validation) \n",
    "    validation_ry.append(y_validation)\n",
    "    \n",
    "print(\"split shakespere done\")\n",
    "\n",
    "print(\"training\")\n",
    "\n",
    "print(training_rx[0])\n",
    "\n",
    "for i in range(len(training_sx)):\n",
    "    training_global_x = (np.concatenate((training_sx[i], training_rx[i]), axis=0))\n",
    "    training_global_y = (np.concatenate((training_sy[i], training_ry[i]), axis=0))\n",
    "    validation_global_x = (np.concatenate((validation_sx[i], validation_rx[i]), axis=0))\n",
    "    validation_global_y = (np.concatenate((validation_sy[i], validation_ry[i]), axis=0))\n",
    "\n",
    "    \n",
    "    clf = BernoulliNB()\n",
    "    clf.fit(training_global_x,training_global_y)\n",
    "        \n",
    "    pred = (clf.predict(validation_global_x))\n",
    "#     clf.score(training_global_x, training_global_y)\n",
    "    print(clf.score(validation_global_x, validation_global_y))\n",
    "    \n",
    "    if (i == 3):\n",
    "        print(np.where((pred==validation_global_y)==False))\n",
    "    differencesB.append(difference(pred, validation_global_y) / len(validation_global_y))\n",
    "\n",
    "\n",
    "    \n",
    "differencesB\n",
    "# fold_error.append(cross_validation_error(5, splits))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing test documents to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating test documents to run classificaiton on\n",
    "all_files = reuters.fileids()\n",
    "\n",
    "# vectors of document tokens\n",
    "test_vectorR = [w.lower() for w in reuters.words(all_files[0])]\n",
    "\n",
    "test_vectorS = [w.lower() for w in SHAKESPEARE_WORDS[0]]\n",
    "\n",
    "test_mergedS =  [w.lower() for w in SHAKESPEARE_WORDS[1]]\n",
    "\n",
    "test_mergedS.extend(test_vectorS)\n",
    "\n",
    "test_mergedS = np.random.choice(test_mergedS, len(SHAKESPEARE_WORDS[1]))\n",
    "\n",
    "\n",
    "sparse_R = np.zeros(vocab.shape)\n",
    "sparse_S = np.zeros(vocab.shape)\n",
    "sparse_M = np.zeros(vocab.shape)\n",
    "\n",
    "# vectorizing array of tokens\n",
    "for w in test_vectorR:\n",
    "    i, = np.where(vocab == w)\n",
    "    sparse_R[i] = 1\n",
    "    \n",
    "for w in test_vectorS:\n",
    "    i, = np.where(vocab == w)\n",
    "    sparse_S[i] = 1\n",
    "    \n",
    "for w in test_mergedS:\n",
    "    i, = np.where(vocab == w)\n",
    "    sparse_M[i] = 1\n",
    "\n",
    "# sanity check\n",
    "print(sparse_M.shape)\n",
    "print(sparse_M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying the prepared test documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s = shakespeare_data\n",
    "X_r = reuters_data\n",
    "\n",
    "Y_s = np.zeros(X_s.shape[0])\n",
    "Y_r = np.ones(X_r.shape[0])\n",
    "\n",
    "X = np.concatenate((X_s, X_r), axis=0)\n",
    "Y = np.concatenate((Y_s, Y_r), axis=0)\n",
    "\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X,Y)\n",
    "\n",
    "pred = (clf.predict([sparse_M]))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rare Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Rate vs Threshold [work in progress]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_values = [100, 1000, 2000, 3000, 5000]\n",
    "\n",
    "# test_vector -> tokenized doc\n",
    "# vocab -> merged vocab\n",
    "def vectorize(test_vector,vocab):\n",
    "    sparse = np.zeros(vocab.shape)\n",
    "    for w in test_vector:\n",
    "        i, = np.where(vocab == w)\n",
    "        sparse[i] = 1\n",
    "    return sparse\n",
    "        \n",
    "        \n",
    "for threshold in threshold_values:\n",
    "    top_s= shakespeare_vocab_np[:threshold]\n",
    "    top_r = reuters_vocab_np[:threshold]\n",
    "    \n",
    "    intersect = np.intersect1d(top_s, top_r)\n",
    "    vocab = np.concatenate((top_s, top_r), axis=0)\n",
    "    vocab = np.array([v for v in vocab if v not in intersect])\n",
    "    \n",
    "    for row in shakespeare_data:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yellowbrick'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-d2838dbc5788>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0myellowbrick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTSNEVisualizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'yellowbrick'"
     ]
    }
   ],
   "source": [
    "from yellowbrick.text import TSNEVisualizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
