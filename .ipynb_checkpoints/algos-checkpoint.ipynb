{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello', 'my', 'name', 'is', 'Daniel']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download()\n",
    "word_tokenize(\"Hello my name is Daniel\")\n",
    "\n",
    "# def tokenizeDoc(fileName):\n",
    "#     with open(fileName) as fp :\n",
    "#         return [w.lower() for w in nltk.word_tokenize(fp.read()) if w.lower() not in stopwords.words(\"english\")]\n",
    "# nltk.download()\n",
    "# article1 = tokenizeDoc(\"reuters1.txt\")\n",
    "# print(article1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "\n",
    "\n",
    "def tokenizeDoc(file):\n",
    "    with open(file) as fp:\n",
    "       return [w.lower() for w in word_tokenize(fp.read()) if w.lower() not in stopwords.words(\"english\") and not re.match(r'\\W|_+', w.lower())]\n",
    "\n",
    "def getTop(x, docs):\n",
    "        pass\n",
    "\n",
    "\n",
    "tokenizeDoc(\"reuters1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# article1 = list(set(tokenizeDoc(\"reuters1.txt\")))\n",
    "# article2 = list(set(tokenizeDoc(\"reuters2.txt\")))\n",
    "# article3 = list(set(tokenizeDoc(\"shakespere1.txt\")))\n",
    "# article4 = list(set(tokenizeDoc(\"shakespere2.txt\")))\n",
    "\n",
    "## NGRAMS\n",
    "\n",
    "article1 = list(ngrams(tokenizeDoc(\"reuters1.txt\"), 2))\n",
    "article2 = list(ngrams(tokenizeDoc(\"reuters2.txt\"), 2))\n",
    "article3 = list(ngrams(tokenizeDoc(\"shakespere1.txt\"), 2))\n",
    "article4 = list(ngrams(tokenizeDoc(\"shakespere2.txt\"), 2))\n",
    "\n",
    "intersectionR = set.intersection(set(article1), set(article2))\n",
    "intersectionS = set.intersection(set(article3), set(article4))\n",
    "\n",
    "intersectionRG = set.intersection(set(article1), set(article3))\n",
    "intersectionSG = set.intersection(set(article2), set(article4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article1 = tokenizeDoc(\"reuters1.txt\")\n",
    "article2 = tokenizeDoc(\"reuters2.txt\")\n",
    "article3 = tokenizeDoc(\"shakespere1.txt\")\n",
    "article4 = tokenizeDoc(\"shakespere2.txt\")\n",
    "\n",
    "Y = [0,0,1,1]\n",
    "\n",
    "test = tokenizeDoc(\"test.txt\")\n",
    "\n",
    "headers = article1 + article2 + article3 + article4\n",
    "\n",
    "vec1 = []\n",
    "vec2 = []\n",
    "vec3 = []\n",
    "vec4 = []\n",
    "\n",
    "for word in headers:\n",
    "    if word in article1:\n",
    "        vec1.append(1)\n",
    "    else:\n",
    "        vec1.append(0)\n",
    "\n",
    "for word in headers:\n",
    "    if word in article2:\n",
    "        vec2.append(1)\n",
    "    else:\n",
    "        vec2.append(0)\n",
    "\n",
    "for word in headers:\n",
    "    if word in article3:\n",
    "        vec3.append(1)\n",
    "    else:\n",
    "        vec3.append(0)\n",
    "        \n",
    "for word in headers:\n",
    "    if word in article4:\n",
    "        vec4.append(1)\n",
    "    else:\n",
    "        vec4.append(0)\n",
    "\n",
    "X = [vec1,vec2,vec3,vec4] \n",
    "#shapespere = 0, reuters = 1\n",
    "Y = [0,0,1,1] \n",
    "\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X,Y)\n",
    "\n",
    "print(clf.predict(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "doc1 = [1,1,0,0,0]\n",
    "doc2 = [1,0,0,0,1] \n",
    "doc3 = [0,0,1,1,0]\n",
    "doc4 = [0,0,1,1,1]\n",
    "test = [0,0,1,0,0] \n",
    "\n",
    "X = [doc1,doc2,doc3,doc4] \n",
    "#shapespere = 0, reuters = 1\n",
    "Y = [0,0,1,1] \n",
    "\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X,Y)\n",
    "\n",
    "print(clf.predict([test]))\n",
    "\n",
    "clfb = GaussianNB()\n",
    "clfb.fit(X,Y)\n",
    "\n",
    "print(clfb.predict([test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def difference(l1,l2):\n",
    "    precisionError = len([b for a,b in zip(l1,l2) if b != a and b == 0 and a == 1])\n",
    "    recallError = len([b for a,b in zip(l1,l2) if b != a and b == 1 and a == 0])\n",
    "    print(\"Precision Error is \" + str(precisionError))\n",
    "    print(\"Recall Error is \" + str(recallError))\n",
    "\n",
    "    return (precisionError) + (recallError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runModels(Xtr, Ytr):\n",
    "    kf = KFold(n_splits=2)    \n",
    "    differencesB = []\n",
    "    differencesG = []\n",
    "\n",
    "    for train_index, test_index in kf.split(Xtr):\n",
    "        x_train, x_test = [Xtr[i] for i in train_index], [Xtr[i] for i in test_index]\n",
    "        y_train, y_test = [Ytr[i] for i in train_index], [Ytr[i] for i in test_index]\n",
    "        \n",
    "        clf = BernoulliNB()\n",
    "        clf.fit(x_train,y_train)\n",
    "        \n",
    "        print(clf.predict(x_test))\n",
    "        differencesB.append(difference(clf.predict(x_test), y_test) / len(y_test))\n",
    "\n",
    "        clfb = GaussianNB()\n",
    "        clfb.fit(X,Y)\n",
    "        \n",
    "        print(clfb.predict(x_test))\n",
    "        differencesG.append(difference(clfb.predict(x_test), y_test) / len(y_test))\n",
    "\n",
    "    return differencesB, differencesG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n",
      "Precision Error is 2\n",
      "Recall Error is 0\n",
      "[0 0]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "[0 0]\n",
      "Precision Error is 0\n",
      "Recall Error is 2\n",
      "[1 1]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "([1.0, 1.0], [0.0, 0.0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\davie\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    }
   ],
   "source": [
    "print(runModels(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
