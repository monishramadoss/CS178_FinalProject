{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\monish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\monish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\monish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from concurrent import futures\n",
    "import nltk\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from scipy import sparse\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "NUM_THREADS = 24s\n",
    "\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "reuters_freqThreshold = 50000\n",
    "shakespeare_freqThreshold = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuters Data Scrubbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10788\n"
     ]
    }
   ],
   "source": [
    "all_files = reuters.fileids()\n",
    "file_count = len(all_files)\n",
    "print(len(reuters.fileids()))\n",
    "punctuation = string.punctuation.replace(\"'\", \"\")\n",
    "stopset = set(stopwords.words(\"english\"))\n",
    "\n",
    "def thread_task(rawWords):\n",
    "    freqMap = defaultdict(int)\n",
    "    #convert to lowercase\n",
    "    lower = [word.lower() for word in rawWords]\n",
    "    \n",
    "    #remove punctuation from tokens\n",
    "    punc_filter = str.maketrans('', '', punctuation)\n",
    "    stripped = [word.translate(punc_filter) for word in lower]\n",
    "    \n",
    "    #remove remaining alphanumerics\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    #filter out basic stopwords\n",
    "    cleaned_word_bank = [word for word in words if word not in stopset]\n",
    "    \n",
    "    for word in cleaned_word_bank:\n",
    "        freqMap[word] += 1\n",
    "    return freqMap\n",
    "\n",
    "def thread_exec(WORDS):\n",
    "    with futures.ThreadPoolExecutor(max_workers=NUM_THREADS) as ex:\n",
    "        results = list(ex.map(thread_task, WORDS))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29027,)\n"
     ]
    }
   ],
   "source": [
    "freqMap = defaultdict(int)\n",
    "REUTERS_WORDS = [[str(word) for word in reuters.words(file)] for file in reuters.fileids()]\n",
    "t = thread_exec(REUTERS_WORDS)\n",
    "for ifreqMap in t:\n",
    "    for k in ifreqMap.keys():    \n",
    "        freqMap[k] += ifreqMap[k]    \n",
    "        \n",
    "freqTuples = list(freqMap.items())\n",
    "sorted_freq = freqTuples.sort(key= lambda x: x[1], reverse=True)\n",
    "topTuples = freqTuples[:reuters_freqThreshold]\n",
    "filtered_vocab = [tup[0] for tup in topTuples]\n",
    "reuters_vocab_np = np.asarray(filtered_vocab, dtype='str')\n",
    "print(reuters_vocab_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shakespeare Data Scrubbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './works'\n",
    "sub = \"[^a-zA-Z' ]+\"\n",
    "all_files = [file for t,y, file in os.walk(folder)][0]\n",
    "stopset = set(stopwords.words(\"english\"))\n",
    "\n",
    "def thread_task(rawWords):\n",
    "    freqMap = defaultdict(int)\n",
    "    words = [re.sub(sub, '', word) for word in rawWords]\n",
    "    lower = [word.lower() for word in words]\n",
    "    words = [word for word in lower if word.isalpha()]\n",
    "    cleaned_word_bank = [word for word in words if word not in stopset]    \n",
    "    \n",
    "    for word in cleaned_word_bank:\n",
    "        freqMap[word] += 1\n",
    "    return freqMap\n",
    "\n",
    "def thread_exec(WORDS):\n",
    "    with futures.ThreadPoolExecutor(max_workers=NUM_THREADS) as ex:\n",
    "        results = list(ex.map(thread_task, WORDS))\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29027,)\n"
     ]
    }
   ],
   "source": [
    "freqMap = defaultdict(int)\n",
    "SHAKESPEARE_WORDS = [[str(word) for word in open(folder+'/'+file).read().split()] for file in all_files]\n",
    "t = thread_exec(SHAKESPEARE_WORDS)\n",
    "for ifreqMap in t:\n",
    "    for k in ifreqMap.keys():    \n",
    "        freqMap[k] += ifreqMap[k]   \n",
    "        \n",
    "freqTuples = list(freqMap.items())\n",
    "sorted_freq = freqTuples.sort(key= lambda x: x[1], reverse=True)\n",
    "topTuples = freqTuples[:shakespeare_freqThreshold]\n",
    "filtered_vocab2 = [tup[0] for tup in topTuples]\n",
    "shakespeare_vocab_np = np.asarray(filtered_vocab, dtype='str')\n",
    "print(shakespeare_vocab_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['said' 'mln' 'vs' ... 'genecor' 'additivies' 'krn']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c9a0b7cf944c73a72f9d620eac59ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=42)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 29027)\n",
      "['said' 'mln' 'vs' ... 'genecor' 'additivies' 'krn']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3724982e84a945669641f826a51a35a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=10788)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10788, 29027)\n"
     ]
    }
   ],
   "source": [
    "from ipywidgets import IntProgress, HTML, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "vocab = reuters_vocab_np\n",
    "for word in shakespeare_vocab_np:\n",
    "    if(word not in vocab):\n",
    "        vocab.append(word)\n",
    "print(vocab.shape)\n",
    "\n",
    "\n",
    "shakespeare_data = []\n",
    "i=0\n",
    "punc_filter = str.maketrans('', '', punctuation)\n",
    "progress = IntProgress(min=0, max=len(SHAKESPEARE_WORDS))\n",
    "label = HTML()\n",
    "box = VBox(children=[label, progress])\n",
    "display(box)\n",
    "for doc in SHAKESPEARE_WORDS:\n",
    "    shakespeare_data.append(np.zeros(vocab.shape))\n",
    "    #print(doc)\n",
    "    for word in doc:\n",
    "        word = re.sub(sub,'',word).lower()\n",
    "        shakespeare_data[i][vocab==word] = 1.0\n",
    "    i+=1\n",
    "    progress.value += 1\n",
    "    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=\"Docs\",\n",
    "                        index=i,\n",
    "                        size=len(SHAKESPEARE_WORDS)\n",
    "                    )\n",
    "shakespeare_data = np.array(shakespeare_data)   \n",
    "\n",
    "\n",
    "reuters_data = []\n",
    "i=0\n",
    "punc_filter = str.maketrans('', '', punctuation)\n",
    "progress = IntProgress(min=0, max=len(REUTERS_WORDS))\n",
    "label = HTML()\n",
    "box = VBox(children=[label, progress])\n",
    "display(box)\n",
    "for doc in REUTERS_WORDS:\n",
    "    reuters_data.append(np.zeros(vocab.shape))\n",
    "    for word in doc:\n",
    "        word = word.lower()\n",
    "        word = word.translate(punc_filter)\n",
    "        reuters_data[i][vocab==word] = 1.0\n",
    "    i+=1   \n",
    "    progress.value += 1\n",
    "    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=\"Docs\",\n",
    "                        index=i,\n",
    "                        size=len(REUTERS_WORDS)\n",
    "                    )\n",
    "reuters_data = np.array(reuters_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10830, 29027)\n",
      "(10830,)\n"
     ]
    }
   ],
   "source": [
    "X = shakespeare_data\n",
    "y = np.zeros(X.shape[0])\n",
    "\n",
    "X = np.concatenate((X, reuters_data), axis=0)\n",
    "y = np.concatenate((y, np.zeros(reuters_data.shape[0])), axis=0)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X.dump('X')\n",
    "y.dump('y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('X')\n",
    "y = np.load('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(l1,l2):\n",
    "    precisionError = len([b for a,b in zip(l1,l2) if b != a and b == 0 and a == 1])\n",
    "    recallError = len([b for a,b in zip(l1,l2) if b != a and b == 1 and a == 0])\n",
    "    print(\"Precision Error is \" + str(precisionError))\n",
    "    print(\"Recall Error is \" + str(recallError))\n",
    "\n",
    "    return (precisionError) + (recallError)\n",
    "\n",
    "def runModels(Xtr, Ytr):\n",
    "    kf = KFold(n_splits=5)    \n",
    "    differencesB = []\n",
    "    differencesG = []\n",
    "\n",
    "    for train_index, test_index in kf.split(Xtr):\n",
    "        x_train, x_test = [Xtr[i] for i in train_index], [Xtr[i] for i in test_index]\n",
    "        y_train, y_test = [Ytr[i] for i in train_index], [Ytr[i] for i in test_index]\n",
    "        \n",
    "        clf = BernoulliNB()\n",
    "        clf.fit(x_train,y_train)\n",
    "        \n",
    "        print(clf.predict(x_test))\n",
    "        differencesB.append(difference(clf.predict(x_test), y_test) / len(y_test))\n",
    "\n",
    "        clfb = GaussianNB()\n",
    "        clfb.fit(x_train,y_train)\n",
    "        \n",
    "        print(clfb.predict(x_test))\n",
    "        differencesG.append(difference(clfb.predict(x_test), y_test) / len(y_test))\n",
    "\n",
    "    return differencesB, differencesG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Microsoft\\VisualStudio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "([0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0])\n"
     ]
    }
   ],
   "source": [
    "print(runModels(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
