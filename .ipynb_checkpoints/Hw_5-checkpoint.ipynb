{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\davie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\davie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\davie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from concurrent import futures\n",
    "import nltk\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from scipy import sparse\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "NUM_THREADS = 24\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "reuters_freqThreshold = 5000\n",
    "shakespeare_freqThreshold = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuters Data Scrubbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10788\n"
     ]
    }
   ],
   "source": [
    "all_files = reuters.fileids()\n",
    "file_count = len(all_files)\n",
    "print(len(reuters.fileids()))\n",
    "punctuation = string.punctuation.replace(\"'\", \"\")\n",
    "stopset = set(stopwords.words(\"english\"))\n",
    "\n",
    "def thread_task(rawWords):\n",
    "    freqMap = defaultdict(int)\n",
    "    #convert to lowercase\n",
    "    lower = [word.lower() for word in rawWords]\n",
    "    \n",
    "    #remove punctuation from tokens\n",
    "    punc_filter = str.maketrans('', '', punctuation)\n",
    "    stripped = [word.translate(punc_filter) for word in lower]\n",
    "    \n",
    "    #remove remaining alphanumerics\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    #filter out basic stopwords\n",
    "    cleaned_word_bank = [word for word in words if word not in stopset]\n",
    "    \n",
    "    for word in cleaned_word_bank:\n",
    "        freqMap[word] += 1\n",
    "    return freqMap\n",
    "\n",
    "def thread_exec(WORDS):\n",
    "    with futures.ThreadPoolExecutor(max_workers=NUM_THREADS) as ex:\n",
    "        results = list(ex.map(thread_task, WORDS))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "freqMap = defaultdict(int)\n",
    "REUTERS_WORDS = [[str(word) for word in reuters.words(file)] for file in reuters.fileids()]\n",
    "t = thread_exec(REUTERS_WORDS)\n",
    "for ifreqMap in t:\n",
    "    for k in ifreqMap.keys():    \n",
    "        freqMap[k] += ifreqMap[k]    \n",
    "        \n",
    "freqTuples = list(freqMap.items())\n",
    "sorted_freq = freqTuples.sort(key= lambda x: x[1], reverse=True)\n",
    "topTuples_r = freqTuples[:reuters_freqThreshold]\n",
    "filtered_vocab = [tup[0] for tup in topTuples_r]\n",
    "reuters_vocab_np = np.asarray(filtered_vocab, dtype='str')\n",
    "print(reuters_vocab_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shakespeare Data Scrubbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = './Shake_works'\n",
    "sub = \"[^a-zA-Z' ]+\"\n",
    "all_files = [file for t,y, file in os.walk(folder)][0]\n",
    "stopset = set(stopwords.words(\"english\"))\n",
    "\n",
    "def thread_task(rawWords):\n",
    "    freqMap = defaultdict(int)\n",
    "    words = [re.sub(sub, '', word) for word in rawWords]\n",
    "    lower = [word.lower() for word in words]\n",
    "    words = [word for word in lower if word.isalpha()]\n",
    "    cleaned_word_bank = [word for word in words if word not in stopset]    \n",
    "    \n",
    "    for word in cleaned_word_bank:\n",
    "        freqMap[word] += 1\n",
    "    return freqMap\n",
    "\n",
    "def thread_exec(WORDS):\n",
    "    with futures.ThreadPoolExecutor(max_workers=NUM_THREADS) as ex:\n",
    "        results = list(ex.map(thread_task, WORDS))\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "freqMap = defaultdict(int)\n",
    "SHAKESPEARE_WORDS = [[str(word) for word in open(folder+'/'+file).read().split()] for file in all_files]\n",
    "t = thread_exec(SHAKESPEARE_WORDS)\n",
    "for ifreqMap in t:\n",
    "    for k in ifreqMap.keys():    \n",
    "        freqMap[k] += ifreqMap[k]   \n",
    "        \n",
    "freqTuples = list(freqMap.items())\n",
    "sorted_freq = freqTuples.sort(key= lambda x: x[1], reverse=True)\n",
    "topTuples_s = freqTuples[:shakespeare_freqThreshold]\n",
    "filtered_vocab = [tup[0] for tup in topTuples_s]\n",
    "shakespeare_vocab_np = np.asarray(filtered_vocab, dtype='str')\n",
    "print(shakespeare_vocab_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputting top words/frequencies to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuters_freq_file = open(\"reuters_frequencies.txt\", \"w+\")\n",
    "shakes_freq_file = open(\"shakes_frequencies.txt\", \"w+\")\n",
    "\n",
    "for tup in topTuples_r:\n",
    "    reuters_freq_file.write(f\"{tup[0]}, {tup[1]}\")\n",
    "for tup in topTuples_s:\n",
    "    shakes_freq_file.write(f\"{tup[0]}, {tup[1]}\")\n",
    "    \n",
    "reuters_freq_file.close()\n",
    "shakes_freq_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8584,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6826311874594e4d9267d5f1488395d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=42)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070c4b5262fe4c3297703679dcf69584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=10788)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import IntProgress, HTML, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "vocab = reuters_vocab_np\n",
    "tmp = list()\n",
    "for word in shakespeare_vocab_np:\n",
    "    if(word not in vocab):\n",
    "        tmp.append(word)\n",
    "vocab = np.append(vocab, tmp)\n",
    "print(vocab.shape)\n",
    "\n",
    "\n",
    "shakespeare_data = []\n",
    "i=0\n",
    "punc_filter = str.maketrans('', '', punctuation)\n",
    "progress = IntProgress(min=0, max=len(SHAKESPEARE_WORDS))\n",
    "label = HTML()\n",
    "box = VBox(children=[label, progress])\n",
    "display(box)\n",
    "for doc in SHAKESPEARE_WORDS:\n",
    "    shakespeare_data.append(np.zeros(vocab.shape))\n",
    "    #print(doc)\n",
    "    for word in doc:\n",
    "        word = re.sub(sub,'',word).lower()\n",
    "        shakespeare_data[i][vocab==word] = 1.0\n",
    "    i+=1\n",
    "    progress.value += 1\n",
    "    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=\"Docs\",\n",
    "                        index=i,\n",
    "                        size=len(SHAKESPEARE_WORDS)\n",
    "                    )\n",
    "shakespeare_data = np.array(shakespeare_data)   \n",
    "\n",
    "\n",
    "reuters_data = []\n",
    "i=0\n",
    "punc_filter = str.maketrans('', '', punctuation)\n",
    "progress = IntProgress(min=0, max=len(REUTERS_WORDS))\n",
    "label = HTML()\n",
    "box = VBox(children=[label, progress])\n",
    "display(box)\n",
    "for doc in REUTERS_WORDS:\n",
    "    reuters_data.append(np.zeros(vocab.shape))\n",
    "    for word in doc:\n",
    "        word = word.lower()\n",
    "        word = word.translate(punc_filter)\n",
    "        reuters_data[i][vocab==word] = 1.0\n",
    "    i+=1   \n",
    "    progress.value += 1\n",
    "    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=\"Docs\",\n",
    "                        index=i,\n",
    "                        size=len(REUTERS_WORDS)\n",
    "                    )\n",
    "reuters_data = np.array(reuters_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10830, 8584)\n",
      "(10830,)\n"
     ]
    }
   ],
   "source": [
    "X = shakespeare_data\n",
    "y = np.zeros(X.shape[0])\n",
    "\n",
    "X = np.concatenate((X, reuters_data), axis=0)\n",
    "y = np.concatenate((y, np.ones(reuters_data.shape[0])), axis=0)\n",
    "\n",
    "vocab = np.array(vocab, dtype=np.str)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X.dump('X')\n",
    "y.dump('y')\n",
    "vocab.dump('vocab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('X')\n",
    "y = np.load('y')\n",
    "vocab = np.load('vocab')\n",
    "Model_List = list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def difference(l1,l2):\n",
    "    precisionError = len([b for a,b in zip(l1,l2) if b != a and b == 0 and a == 1])\n",
    "    recallError = len([b for a,b in zip(l1,l2) if b != a and b == 1 and a == 0])\n",
    "    print(\"Precision Error is \" + str(precisionError))\n",
    "    print(\"Recall Error is \" + str(recallError))\n",
    "\n",
    "    return (precisionError) + (recallError)\n",
    "\n",
    "def runModels(clf, Xtr, Ytr):\n",
    "    kf = KFold(n_splits=5)    \n",
    "    differencesB = []\n",
    "    differencesG = []\n",
    "\n",
    "    for train_index, test_index in kf.split(Xtr):\n",
    "        x_train, x_test = [Xtr[i] for i in train_index], [Xtr[i] for i in test_index]\n",
    "        y_train, y_test = [Ytr[i] for i in train_index], [Ytr[i] for i in test_index]    \n",
    "        clf.fit(x_train,y_train)\n",
    "        print(clf.predict(x_test))\n",
    "        Model_List.append(clf)\n",
    "        differencesB.append(difference(clf.predict(x_test), y_test) / len(y_test))\n",
    "    print()\n",
    "    return differencesB, differencesG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(runModels(BernoulliNB(), X, y))\n",
    "print(runModels(GaussianNB(), X, y))\n",
    "#print(runModels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seperating start\n",
      "seperated docs\n",
      "splitting shakespere\n",
      "split shakespere done\n",
      "split shakespere done\n",
      "training\n",
      "[[1. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# custom k-fold cross validation code\n",
    "# custom k-fold cross validation code\n",
    "\n",
    "# def k_fold(X, Y1, Y2,folds)\n",
    "#     split_size = X.shape[0]//folds\n",
    "#     splits = []\n",
    "#     for i in range(folds):\n",
    "#         start = split_size*i\n",
    "#         end = (split_size*(i+1))\n",
    "#         validation = (X[start:end,], Y[start:end])\n",
    "#         train_x = np.append(X[:start,], X[end:,])\n",
    "#         train_y = np.append(Y[:start],Y[end:])\n",
    "#         training = (np.atleast_2d(train_x).T, train_y)\n",
    "#         splits.append((training, validation))\n",
    "#     return splits\n",
    "\n",
    "fold_error = []\n",
    "print(\"seperating start\")\n",
    "X_s = X[y==0]\n",
    "X_r = X[y==1]\n",
    "\n",
    "Y_s = np.zeros(X_s.shape[0])\n",
    "Y_r = np.ones(X_r.shape[0])\n",
    "\n",
    "print(\"seperated docs\")\n",
    "\n",
    "## 5 -> 8 shakespere docs\n",
    "## 5 -> ~2k reuters docs\n",
    "folds = KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "splits = []\n",
    "training_sx = []\n",
    "training_sy = []\n",
    "\n",
    "validation_sx = []\n",
    "validation_sy = []\n",
    "\n",
    "\n",
    "training_rx = []\n",
    "training_ry = []\n",
    "validation_rx = []\n",
    "validation_ry = []\n",
    "\n",
    "differencesB = []\n",
    "\n",
    "\n",
    "print(\"splitting shakespere\")\n",
    "for train_index, test_index in folds.split(X_s, Y_s):\n",
    "    x_tr, x_validation = X_s[train_index], X_s[test_index]\n",
    "    y_tr, y_validation = Y_s[train_index], Y_s[test_index]\n",
    "    training_sx.append(x_tr)\n",
    "    training_sy.append(y_tr)\n",
    "    validation_sx.append(x_validation) \n",
    "    validation_sy.append(y_validation)\n",
    "print(\"split shakespeare done\")\n",
    "    \n",
    "for train_index, test_index in folds.split(X_r, Y_r):\n",
    "    x_tr, x_validation = X_r[train_index], X_r[test_index]\n",
    "    y_tr, y_validation = Y_r[train_index], Y_r[test_index]\n",
    "    training_rx.append(x_tr)\n",
    "    training_ry.append(y_tr)\n",
    "    validation_rx.append(x_validation) \n",
    "    validation_ry.append(y_validation)\n",
    "print(\"split shakespere done\")\n",
    "\n",
    "print(\"training\")\n",
    "\n",
    "print(training_rx[0])\n",
    "def validate(clf, name):\n",
    "    for i in range(len(training_sx)):\n",
    "        training_global_x = (np.concatenate((training_sx[i], training_rx[i]), axis=0))\n",
    "        training_global_y = (np.concatenate((training_sy[i], training_ry[i]), axis=0))\n",
    "        validation_global_x = (np.concatenate((validation_sx[i], validation_rx[i]), axis=0))\n",
    "        validation_global_y = (np.concatenate((validation_sy[i], validation_ry[i]), axis=0))\n",
    "\n",
    "        clf.fit(training_global_x,training_global_y)\n",
    "\n",
    "        pred = (clf.predict(validation_global_x))\n",
    "        print(name, clf.score(validation_global_x, validation_global_y))\n",
    "\n",
    "        if (i == 3):\n",
    "            print(np.where((pred==validation_global_y)==False))\n",
    "        differencesB.append(difference(pred, validation_global_y) / len(validation_global_y))\n",
    "    print()\n",
    "\n",
    "# fold_error.append(cross_validation_error(5, splits))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes 1.0\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "Naive Bayes 1.0\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "Naive Bayes 1.0\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "Naive Bayes 0.9995381062355658\n",
      "(array([5], dtype=int64),)\n",
      "Precision Error is 1\n",
      "Recall Error is 0\n",
      "Naive Bayes 1.0\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "\n",
      "G Naive Bayes 1.0\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "G Naive Bayes 1.0\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "G Naive Bayes 1.0\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "G Naive Bayes 0.9995381062355658\n",
      "(array([5], dtype=int64),)\n",
      "Precision Error is 1\n",
      "Recall Error is 0\n",
      "G Naive Bayes 1.0\n",
      "Precision Error is 0\n",
      "Recall Error is 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validate(BernoulliNB(), 'Naive Bayes')\n",
    "validate(GaussianNB(), 'G Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing test documents to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-4af263eab9fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0msparse_R\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0msparse_S\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0msparse_M\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "## creating test documents to run classificaiton on\n",
    "all_files = reuters.fileids()\n",
    "\n",
    "\n",
    "# vectors of document tokens\n",
    "test_vectorR = [w.lower() for w in reuters.words(all_files[0])]\n",
    "\n",
    "test_vectorS = [w.lower() for w in SHAKESPEARE_WORDS[0]]\n",
    "\n",
    "test_mergedS =  [w.lower() for w in SHAKESPEARE_WORDS[1]]\n",
    "\n",
    "test_mergedS.extend(test_vectorS)\n",
    "\n",
    "test_mergedS = np.random.choice(test_mergedS, len(SHAKESPEARE_WORDS[1]))\n",
    "\n",
    "\n",
    "sparse_R = np.zeros(vocab.shape)\n",
    "sparse_S = np.zeros(vocab.shape)\n",
    "sparse_M = np.zeros(vocab.shape)\n",
    "\n",
    "# vectorizing array of tokens\n",
    "for w in test_vectorR:\n",
    "    i, = np.where(vocab == w)\n",
    "    sparse_R[i] = 1\n",
    "    \n",
    "for w in test_vectorS:\n",
    "    i, = np.where(vocab == w)\n",
    "    sparse_S[i] = 1\n",
    "    \n",
    "for w in test_mergedS:\n",
    "    i, = np.where(vocab == w)\n",
    "    sparse_M[i] = 1\n",
    "\n",
    "# sanity check\n",
    "print(sparse_M.shape)\n",
    "print(sparse_M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying the prepared test documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_s = shakespeare_data\n",
    "X_r = reuters_data\n",
    "\n",
    "Y_s = np.zeros(X_s.shape[0])\n",
    "Y_r = np.ones(X_r.shape[0])\n",
    "\n",
    "X = np.concatenate((X_s, X_r), axis=0)\n",
    "Y = np.concatenate((Y_s, Y_r), axis=0)\n",
    "\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X,Y)\n",
    "\n",
    "pred = (clf.predict([sparse_M]))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rare Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Rate vs Threshold [work in progress]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold_values = [100, 1000, 2000, 3000, 5000]\n",
    "\n",
    "# test_vector -> tokenized doc\n",
    "# vocab -> merged vocab\n",
    "def vectorize(test_vector,vocab):\n",
    "    sparse = np.zeros(vocab.shape)\n",
    "    for w in test_vector:\n",
    "        i, = np.where(vocab == w)\n",
    "        sparse[i] = 1\n",
    "    return sparse\n",
    "        \n",
    "        \n",
    "for threshold in threshold_values:\n",
    "    top_s= shakespeare_vocab_np[:threshold]\n",
    "    top_r = reuters_vocab_np[:threshold]\n",
    "    \n",
    "    intersect = np.intersect1d(top_s, top_r)\n",
    "    vocab = np.concatenate((top_s, top_r), axis=0)\n",
    "    vocab = np.array([v for v in vocab if v not in intersect])\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE Corpus Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_words(raw_words: 'list of str'):\n",
    "    '''Filters raw word list then returns single string version'''\n",
    "    #convert to lowercase\n",
    "    lower = [word.lower() for word in raw_words]\n",
    "    \n",
    "    #remove punctuation from tokens\n",
    "    punc_filter = str.maketrans('', '', punctuation)\n",
    "    stripped = [word.translate(punc_filter) for word in lower]\n",
    "    \n",
    "    #remove remaining alphanumerics\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    #filter out basic stopwords\n",
    "    cleaned_word_bank = [word + ' ' for word in words if word not in stopset]\n",
    "    return ''.join(cleaned_word_bank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from yellowbrick.text import TSNEVisualizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets.base import Bunch\n",
    "from yellowbrick.style import set_palette\n",
    "\n",
    "folder = './Shake_works'\n",
    "all_files = [file for t,y, file in os.walk(folder)][0]\n",
    "punctuation = string.punctuation.replace(\"'\", \"\")\n",
    "stopset = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Load raw text data and labels simultaneously\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "subset = np.random.choice(reuters.fileids(), 158)\n",
    "# Reuters\n",
    "for file in subset:\n",
    "    raw_words = [str(word) for word in reuters.words(file)]\n",
    "    processed = process_words(raw_words)\n",
    "    data.append(processed)\n",
    "    labels.append('Reuters')\n",
    "    \n",
    "# Shakespeare\n",
    "for file in all_files:\n",
    "    raw_words = [str(word) for word in open(folder+'/'+file).read().split()]\n",
    "    processed = process_words(raw_words)\n",
    "    data.append(processed)\n",
    "    labels.append('Shakespeare')\n",
    "\n",
    "corpus = Bunch(\n",
    "    data=data,\n",
    "    target=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit tf-idf vectorizer\n",
    "TF_IDF = TfidfVectorizer()\n",
    "docs = TF_IDF.fit_transform(corpus.data)\n",
    "\n",
    "# Set color palette for plotting\n",
    "set_palette('sns_dark')\n",
    "\n",
    "# Construct visualizer and draw vectors\n",
    "tsne = TSNEVisualizer(colors=('r','b'))\n",
    "tsne.fit(docs, corpus.target)\n",
    "tsne.poof()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "SVD = TruncatedSVD(n_components=10, n_iter=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(clf, model, doc):\n",
    "    print(\"Prediction for model: \" + model)\n",
    "    print(clf.predict([doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "def runCentury(century, clf, desc):\n",
    "    print(\"Classification of documents from the \" + century)\n",
    "\n",
    "    onlyfiles = [f for f in listdir(\"TextData/\"+century) if isfile(join(\"TextData/\"+century, f))]\n",
    "\n",
    "    for f in onlyfiles:\n",
    "        doc = []\n",
    "        with open(filename, encoding=\"utf8\") as f:\n",
    "            doc = vectorize(f, vocab)\n",
    "\n",
    "        predict(clf, desc, doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfBernoulli = BernoulliNB()\n",
    "clfBernoulli.fit(X,y)\n",
    "\n",
    "clfGaussian = GaussianNB()\n",
    "clfGaussian.fit(X,y)\n",
    "\n",
    "clfLR = LogisticRegression()\n",
    "clfLR.fit(X,y)\n",
    "\n",
    "clfKnn = KNeighborsClassifier(n_neighbors=5)\n",
    "clfKnn.fit(X,y)\n",
    "\n",
    "clfSVM = svm.SVC()\n",
    "clfSVM.fit(X,y)\n",
    "\n",
    "models = [(clfBernoulli, \"Bernoulli Naive Bayes\"), (clfGaussian, \"Gaussian Naive Bayes\"), (clfLR, \"Logistic Regression\"), (clfKnn, \"K Nearest Neighbours\"), (clfSVM, \"Support Vector Machine\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes\n",
      "Classification of documents from the 1600s\n",
      "Prediction for model: Bernoulli Naive Bayes\n",
      "[ 1.]\n",
      "Prediction for model: Bernoulli Naive Bayes\n",
      "[ 1.]\n",
      "Prediction for model: Bernoulli Naive Bayes\n",
      "[ 1.]\n",
      "Prediction for model: Bernoulli Naive Bayes\n",
      "[ 1.]\n",
      "Gaussian Naive Bayes\n",
      "Classification of documents from the 1600s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-bb1ce6099d51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mrunCentury\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-72-724ee5348711>\u001b[0m in \u001b[0;36mrunCentury\u001b[1;34m(century, clf, desc)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-6f22f454428e>\u001b[0m in \u001b[0;36mvectorize\u001b[1;34m(test_vector, vocab)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_vector\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0msparse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "centuries = [\"1600s\",\"1700s\",\"1800s\",\"1900s\",\"2000s\"]\n",
    "for c in centuries:\n",
    "    for clf in models:\n",
    "        runCentury(c,clf[0],clf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
